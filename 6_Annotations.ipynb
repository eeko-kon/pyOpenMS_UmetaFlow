{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from pyteomics import mgf, auxiliary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= os.path.join(\"results\", \"interim\", \"analysis\")\n",
    "isExist= os.path.exists(path)\n",
    "if not isExist:\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First,import the feature table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureMatrix= os.path.join(\"results\", \"features\", \"FeatureMatrix.tsv\")\n",
    "DF_features= pd.read_csv(FeatureMatrix, sep=\"\\t\")\n",
    "DF_features= DF_features.drop(columns=[\"charge\", \"quality\"])\n",
    "DF_features= DF_features.fillna(0)\n",
    "DF_features[\"feature_ids\"]= DF_features[\"feature_ids\"].str.replace(r\"[\", \"\")\n",
    "DF_features[\"feature_ids\"]= DF_features[\"feature_ids\"].str.replace(r\"]\", \"\")\n",
    "\n",
    "for i, rows in DF_features.iterrows():\n",
    "    DF_features[\"feature_ids\"][i]= DF_features[\"feature_ids\"][i].split(\",\")\n",
    "DF_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `1) SIRIUS and CSI:FingerID annotations`\n",
    "\n",
    "Create a matrix with all SIRIUS and CSI:FingerID formula and structural predictions, only choose #1 rankings predictions and combine the dataframes to annotate formula and structural predictions according to RT and mz:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_formulas will likely contain duplicate formulas that could be either isomeric, isobaric compounds, or identical compounds (with identical RT and mz). Here, we want to collapse the identical, repeating compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_formulas= glob.glob(os.path.join(\"results\", \"SiriusCSI\", \"formulas_*.tsv\"))\n",
    "DF_SIRIUS = pd.DataFrame()\n",
    "list_of_df=[]\n",
    "for csv in input_formulas:\n",
    "    df= pd.read_csv(csv, sep=\"\\t\", index_col=\"Unnamed: 0\")\n",
    "    s= df[\"opt_global_rank\"]\n",
    "    pd.to_numeric(s)\n",
    "    df= df.loc[df[\"opt_global_rank\"]==1]\n",
    "    df= df.rename(columns={\"opt_global_featureId\":\"featureId\"})\n",
    "    df= df.drop(columns=df.filter(regex=fr\"Score\").columns)\n",
    "    df= df.drop(columns= df.filter(regex=fr\"opt\").columns)\n",
    "    df=df.reset_index()\n",
    "    list_of_df.append(df)\n",
    "DF_SIRIUS= pd.concat(list_of_df,ignore_index=True)\n",
    "DF_SIRIUS= DF_SIRIUS.drop(columns=\"index\")\n",
    "DF_SIRIUS= DF_SIRIUS.rename(columns= {\"chemical_formula\": \"formulas\", \"exp_mass_to_charge\": \"mz\", \"retention_time\": \"RT\"})\n",
    "DF_SIRIUS[\"featureId\"]= DF_SIRIUS[\"featureId\"].str.replace(r\"id_\", \"\")\n",
    "DF_SIRIUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for structral predictions (remove duplicates with the same inchi_keys, which means they represent the same structure):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_structures= glob.glob(os.path.join(\"results\", \"SiriusCSI\", \"structures_*.tsv\"))\n",
    "DF_CSI = pd.DataFrame()\n",
    "list_of_df=[]\n",
    "for csv in input_structures:\n",
    "    df= pd.read_csv(csv, sep=\"\\t\", index_col=\"Unnamed: 0\")\n",
    "    s= df[\"opt_global_rank\"]\n",
    "    pd.to_numeric(s)\n",
    "    df= df.loc[df[\"opt_global_rank\"]==1]\n",
    "    df= df.rename(columns={\"opt_global_featureId\":\"featureId\"})\n",
    "    df= df.drop(columns=df.filter(regex=fr\"Score\").columns)\n",
    "    df= df.drop(columns= df.filter(regex=fr\"opt\").columns)\n",
    "    df=df.reset_index()\n",
    "    list_of_df.append(df)\n",
    "DF_CSI= pd.concat(list_of_df,ignore_index=True)\n",
    "DF_CSI= DF_CSI.drop(columns=\"index\")\n",
    "DF_CSI= DF_CSI.rename(columns= {\"chemical_formula\": \"formulas\", \"exp_mass_to_charge\": \"mz\", \"retention_time\": \"RT\", \"description\":\"name\"})\n",
    "DF_CSI[\"featureId\"]= DF_CSI[\"featureId\"].str.replace(r\"id_\", \"\")\n",
    "DF_CSI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotate the formulas and structural predictions to the feature matrix according to SIRIUS and CSI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_features.insert(0, \"SIRIUS_predictions\", \"\")\n",
    "\n",
    "for i, id in zip(DF_features.index, DF_features[\"feature_ids\"]):\n",
    "    hits = []\n",
    "    for name, Pred_id in zip(DF_SIRIUS[\"formulas\"], DF_SIRIUS[\"featureId\"]): \n",
    "        if Pred_id in id:\n",
    "            hit = f\"{name}\"\n",
    "            if hit not in hits:\n",
    "                hits.append(hit)\n",
    "    DF_features[\"SIRIUS_predictions\"][i] = \" ## \".join(hits)\n",
    "DF_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_features.insert(0, \"CSI_predictions_name\", \"\")\n",
    "DF_features.insert(0, \"CSI_predictions_formula\", \"\")\n",
    "DF_features.insert(0, \"CSI_predictions_smiles\", \"\")\n",
    "\n",
    "for i, id, sirius in zip(DF_features.index, DF_features[\"feature_ids\"], DF_features[\"SIRIUS_predictions\"]):\n",
    "    hits1 = []\n",
    "    hits2= []\n",
    "    hits3=[]\n",
    "    for name, formula, smiles, Pred_id in zip(DF_CSI[\"name\"], DF_CSI[\"formulas\"], DF_CSI[\"smiles\"], DF_CSI[\"featureId\"]): \n",
    "        if (Pred_id in id) & (formula in sirius):\n",
    "                hit1 = f\"{name}\"\n",
    "                hit2 = f\"{formula}\"\n",
    "                hit3= f\"{smiles}\"\n",
    "                if hit1 not in hits1:\n",
    "                    hits1.append(hit1)\n",
    "                    hits2.append(hit2)\n",
    "                    hits3.append(hit3)\n",
    "    DF_features[\"CSI_predictions_name\"][i] = \" ## \".join(hits1)\n",
    "    DF_features[\"CSI_predictions_formula\"][i] = \" ## \".join(hits2)\n",
    "    DF_features[\"CSI_predictions_smiles\"][i] = \" ## \".join(hits3)\n",
    "DF_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_features.to_csv(os.path.join(\"results\", \"annotations\", \"SiriusCSI_annotated_FeatureMatrix.tsv\"), sep=\"\\t\", index= None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `2) GNPS annotations`\n",
    "\n",
    "This step requires the file under the DB_result directory (downloaded cytoscape data) after FBMN. Move the .TSV table under the resources directory and annotate the FeatureMatrix according to SCAN numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list= glob.glob(os.path.join(\"resources\", \"*.tsv\"))\n",
    "for file in list:   \n",
    "    df= pd.read_csv(file, sep=\"\\t\")\n",
    "    df.drop(df.index[df['IonMode'] == \"negative\"], inplace=True)\n",
    "    df.drop(df.index[df['MZErrorPPM'] > 10.0], inplace=True)\n",
    "    GNPS=df.drop_duplicates(subset=\"Compound_Name\", keep='first')\n",
    "    GNPS[\"#Scan#\"]= GNPS[\"#Scan#\"].astype(str)\n",
    "GNPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotate the features detected by GNPS according to mz and RT (mz tolerance 10 ppm and RT tolerance 20 seconds: instrument and method-dependent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= os.path.join(\"results\", \"annotations\")\n",
    "isExist= os.path.exists(path)\n",
    "if not isExist:\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the SCAN number information to the annotated FeatureMatrix in order to match the GNPS MSMS library matching results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrix= pd.read_csv(os.path.join(\"results\", \"annotations\", \"SiriusCSI_annotated_FeatureMatrix.tsv\"), sep=\"\\t\")\n",
    "Matrix[\"id\"]= Matrix[\"id\"].astype(str)\n",
    "Matrix[\"feature_ids\"]= Matrix[\"feature_ids\"].values.tolist()\n",
    "Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn the MGF file to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= os.path.join(\"results\", \"GNPSexport\", \"MSMS.mgf\")\n",
    "file= mgf.MGF(source=path, use_header=True, convert_arrays=2, read_charges=True, read_ions=False, dtype=None, encoding=None)\n",
    "parameters=[]\n",
    "for spectrum in file:\n",
    "    parameters.append(spectrum['params'])\n",
    "mgf_file= pd.DataFrame(parameters)\n",
    "mgf_file[\"feature_id\"]= mgf_file[\"feature_id\"].str.replace(r\"e_\", \"\")\n",
    "mgf_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add SCANS column to the Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrix.insert(0, \"SCANS\", \"\")\n",
    "for i, id in zip(Matrix.index, Matrix[\"id\"]):\n",
    "    hits = []\n",
    "    for scan, feature_id in zip(mgf_file[\"scans\"], mgf_file[\"feature_id\"]): \n",
    "        if feature_id==id:\n",
    "            hit = f\"{scan}\"\n",
    "            if hit not in hits:\n",
    "                hits.append(hit)\n",
    "    Matrix[\"SCANS\"][i] = \" ## \".join(hits)\n",
    "Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrix.insert(0, \"GNPS\", \"\")\n",
    "\n",
    "for i, scan in zip(Matrix.index, Matrix[\"SCANS\"]):\n",
    "    hits = []\n",
    "    for name, scan_number, in zip(GNPS[\"Compound_Name\"], GNPS[\"#Scan#\"]):\n",
    "        if scan==scan_number:\n",
    "            hit = f\"{name}\"\n",
    "            if hit not in hits:\n",
    "                hits.append(hit)\n",
    "    Matrix[\"GNPS\"][i] = \" ## \".join(hits)\n",
    "\n",
    "Matrix.to_csv(os.path.join(\"results\", \"annotations\", \"GNPS_annotated_FeatureMatrix.tsv\"), sep=\"\\t\", index = False)\n",
    "Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep the unannotated features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrix= Matrix[Matrix.GNPS == \"\"]\n",
    "Matrix= Matrix.drop(columns= \"GNPS\")\n",
    "Matrix= Matrix.set_index([\"RT\", \"mz\"])\n",
    "Matrix_tocsv= Matrix.reset_index()\n",
    "Matrix_tocsv.to_csv(os.path.join(\"results\", \"annotations\", \"Matrix_unknowns.tsv\"), sep=\"\\t\", index =None)\n",
    "Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "edde62aa2661007f0756e9790e7a328c288a583bf6ce768a355147dac67c8db8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pyopenms')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
