{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First,import the feature table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureMatrix= os.path.join(\"results\", \"features\", \"FeatureMatrix_Requantified.tsv\")\n",
    "DF_features= pd.read_csv(FeatureMatrix, sep=\"\\t\")\n",
    "DF_features=DF_features.set_index([\"mz\", \"RT\"])\n",
    "DF_features= DF_features.drop(columns=[\"charge\", \"quality\", \"id\"])\n",
    "DF_features= DF_features.fillna(0)\n",
    "DF_features[\"id_list\"]= DF_features[\"id_list\"].str.replace(r\"{|}|'\", \"\")\n",
    "for i, rows in DF_features.iterrows():\n",
    "    DF_features[\"id_list\"][i]= DF_features[\"id_list\"][i].split(\",\")\n",
    "DF_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `1) Filter the feature matrix (optional)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= os.path.join(\"results\", \"interim\", \"analysis\")\n",
    "isExist= os.path.exists(path)\n",
    "if not isExist:\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) a) Remove all features detected in negative controls (make sure there is no cross-contamination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_list= [r\"ISP2\", r\"FPY12\", r\"DNPM\"] # different media/conditions (treatments)\n",
    "for medium in media_list:\n",
    "    Features_flt=DF_features.filter(regex=fr\"{medium}\")\n",
    "    blanks= Features_flt.filter(regex=\"blank\", axis= 1) \n",
    "    blanks = blanks.fillna(0)\n",
    "    blanks= blanks.transpose()\n",
    "    dictionary = {}\n",
    "    cols= blanks.columns\n",
    "    for i, col in enumerate(cols):\n",
    "        dictionary[i] = np.count_nonzero(blanks[col]) / len(blanks[col])\n",
    "    column_idx = [key for key, value in dictionary.items() if value >= 0.5] #Remove features that appear most frequently (in more than 50% of the samples) in the negative controls\n",
    "    print(dictionary)\n",
    "    blank_features= blanks.iloc[:, column_idx] \n",
    "    cols= blank_features.columns\n",
    "    Features_flt= Features_flt.transpose()\n",
    "    Features_nb= Features_flt.drop(columns= cols)\n",
    "    Features_nb= Features_nb.dropna(how=\"all\")\n",
    "    blanks=blanks.transpose()\n",
    "    blank_cols= blanks.columns\n",
    "    Features_nb= Features_flt.drop(columns=blank_cols)\n",
    "    filename= os.path.join(path, \"No_NC_\"+ medium + \"_DF_features.csv\")\n",
    "    Features_nb.to_csv(filename, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) b) Or if there are multiple replicates, remove only the features detected in more than 50% of all the negative controls (or blanks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_list= [r\"ISP2\", r\"FPY12\", r\"DNPM\"] # different media/conditions (treatments)\n",
    "for medium in media_list:\n",
    "    Features_flt=DF_features.filter(regex=fr\"{medium}\")\n",
    "    blanks= Features_flt.filter(regex=\"blank\", axis= 1) \n",
    "    blanks = blanks.fillna(0)\n",
    "    blanks= blanks.transpose()\n",
    "    cols= blanks.columns\n",
    "    Features_flt= Features_flt.transpose()\n",
    "    Features_nb= Features_flt.drop(columns= cols)\n",
    "    Features_nb= Features_nb.dropna(how=\"all\")\n",
    "    blanks=blanks.transpose()\n",
    "    blank_cols= blanks.columns\n",
    "    Features_flt= Features_flt.transpose()\n",
    "    Features_nb= Features_flt.drop(columns=blank_cols)\n",
    "    filename= os.path.join(path, \"No_NC_\"+ medium + \"_DF_features.csv\")\n",
    "    Features_nb.to_csv(filename, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Replace the features that have lower intensity than 10^4 with NaN (noise for Orbitrap instruments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_flt(csvfile):   \n",
    "    Features= pd.read_csv(csvfile, sep=\"\\t\")\n",
    "    Features= Features.set_index([\"mz\", \"RT\"])\n",
    "    Features= Features.sort_index(axis=1) \n",
    "    cols= Features.columns\n",
    "    Features[cols] = Features[cols].replace({0:np.nan})\n",
    "    Features[Features<10000] = np.nan\n",
    "    Featuresnew=Features.dropna(how=\"all\")\n",
    "    Featuresnew = Featuresnew.fillna(0)\n",
    "    DF= Featuresnew.reset_index()\n",
    "    file_path = os.path.join(os.path.dirname(csvfile), 'noise_thr_' + os.path.basename(csvfile)[6:])\n",
    "    DF.to_csv(file_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvfiles= glob.glob(os.path.join(path, \"No_NC_*.csv\"))\n",
    "for csvfile in csvfiles:\n",
    "    noise_flt(csvfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Replace the presence of a feature with NaN if the feature is present in only 1 out of 3 replicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rep_filter(csvfile):\n",
    "    Features= pd.read_csv(csvfile, sep=\"\\t\", index_col=\"Unnamed: 0\")\n",
    "    Features= Features.set_index([\"mz\", \"RT\"])\n",
    "    Features= Features.sort_index(axis=1)\n",
    "    cols= Features.columns\n",
    "    Features= Features.fillna(0)\n",
    "    Features= Features.transpose()\n",
    "    Features= Features.reset_index()\n",
    "    Features['genomeID']=Features['index'].str.extract(r'(NBC_?\\d*)')\n",
    "    Features['genomeID_MDNA']=Features['index'].str.extract(r'(MDNAWGS?\\d*|MDNA_WGS_?\\d*)')\n",
    "    Features['genomeID']=Features['genomeID'].fillna(Features['genomeID_MDNA'])\n",
    "    Features= Features.drop(columns=[\"genomeID_MDNA\"])\n",
    "    Features=Features.set_index([\"index\"])\n",
    "    Grouped= Features.groupby(\"genomeID\")\n",
    "    DF= Grouped.transform(lambda x: np.nan if np.count_nonzero(x)<2 else x)\n",
    "    DF=DF.transpose()\n",
    "    DF=DF.reset_index()\n",
    "    file_path = os.path.join(os.path.dirname(csvfile), os.path.basename(csvfile)[10:])\n",
    "    DF.to_csv(file_path, sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvfiles= glob.glob(os.path.join(path, \"noise_thr_*.csv\"))\n",
    "for csvfile in csvfiles:\n",
    "    rep_filter(csvfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Merge all tables on mz and RT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrix_FPY12= pd.read_csv(os.path.join(path, \"FPY12_DF_features.csv\"), sep=\"\\t\")\n",
    "Matrix_ISP2= pd.read_csv(os.path.join(path, \"ISP2_DF_features.csv\"), sep=\"\\t\")\n",
    "Matrix_DNPM= pd.read_csv(os.path.join(path, \"DNPM_DF_features.csv\"), sep=\"\\t\")\n",
    "\n",
    "Matrix_ISP2= Matrix_ISP2.set_index([\"mz\", \"RT\"])\n",
    "Matrix_ISP2= Matrix_ISP2.fillna(0)\n",
    "Matrix_ISP2= Matrix_ISP2.sort_index(axis=1)\n",
    "\n",
    "Matrix_FPY12= Matrix_FPY12.set_index([\"mz\", \"RT\"])\n",
    "Matrix_FPY12= Matrix_FPY12.sort_index(axis=1)\n",
    "Matrix_FPY12= Matrix_FPY12.fillna(0)\n",
    "\n",
    "Matrix_DNPM= Matrix_DNPM.set_index([\"mz\", \"RT\"])\n",
    "Matrix_DNPM= Matrix_DNPM.fillna(0)\n",
    "Matrix_DNPM= Matrix_DNPM.sort_index(axis=1)\n",
    "\n",
    "Matrix_ISP2_FPY12= pd.merge(Matrix_FPY12, Matrix_ISP2, on=[\"mz\", \"RT\"], how=\"outer\")\n",
    "Matrix= pd.merge(Matrix_ISP2_FPY12, Matrix_DNPM, on=[\"mz\", \"RT\"],how= \"outer\")\n",
    "cols= Matrix.columns\n",
    "Matrix[cols] = Matrix[cols].replace({0:np.nan})\n",
    "Matrix= Matrix.dropna(how=\"all\")\n",
    "Matrix= Matrix.reset_index()\n",
    "Matrix.to_csv(os.path.join(path, \"Matrix_Clean.csv\"), sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `2) GNPS annotations`\n",
    "\n",
    "This step requires an independent job at GNPS Spectral Library Search (input: raw mzML files)\n",
    "See documentation: https://ccms-ucsd.github.io/GNPSDocumentation/librarysearch/ \n",
    "\n",
    "When the job is finished, import all identifications from GNPS, save the .TSV table under the resources directory and \"clean up\" the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(os.path.join(\"resources\", \"MS2_LIBRARYSEARCH_all_identifications.tsv\"), sep=\"\\t\", encoding=\"latin-1\")\n",
    "df.drop(df.index[df[\"IonMode\"] == \"negative\"], inplace=True)\n",
    "df.drop(df.index[df[\"MZErrorPPM\"] > 20.0], inplace=True)\n",
    "GNPS=df.filter([\"Compound_Name\", \"RT_Query\", \"Precursor_MZ\"])\n",
    "GNPS=GNPS.rename(columns= {\"RT_Query\": \"RetentionTime\"})\n",
    "GNPS=GNPS.drop_duplicates(subset=\"Compound_Name\", keep=\"first\")\n",
    "GNPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureMatrix= pd.read_csv(os.path.join(path, \"Matrix_Clean.csv\"), sep=\"\\t\")\n",
    "FeatureMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotate the features detected by GNPS according to mz and RT (mz tolerance 10 ppm and RT tolerance 20 seconds: instrument and method-dependent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= os.path.join(\"results\", \"annotations\")\n",
    "isExist= os.path.exists(path)\n",
    "if not isExist:\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureMatrix.insert(0, \"GNPS_IDs\", \"\")\n",
    "\n",
    "for i, mz, rt in zip(FeatureMatrix.index, FeatureMatrix[\"mz\"], FeatureMatrix[\"RT\"]):\n",
    "    hits = []\n",
    "    for name, GNPS_mz, GNPS_rt, in zip(GNPS[\"Compound_Name\"], GNPS[\"Precursor_MZ\"], GNPS[\"RetentionTime\"]):\n",
    "        mass_delta = (abs(GNPS_mz-mz)/GNPS_mz)*1000000.0 if GNPS_mz != 0 else np.nan\n",
    "        if (GNPS_rt >= rt-20.0) & (GNPS_rt <= rt+20.0) & (mass_delta<= 10.0):\n",
    "            hit = f\"{name}\"\n",
    "            if hit not in hits:\n",
    "                hits.append(hit)\n",
    "    FeatureMatrix[\"GNPS_IDs\"][i] = \" ## \".join(hits)\n",
    "\n",
    "FeatureMatrix.to_csv(os.path.join(path, \"GNPS_annotated_feature_matrix.tsv\"), sep=\"\\t\", index = False)\n",
    "FeatureMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep the unannotated features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureMatrix= FeatureMatrix[FeatureMatrix.GNPS_IDs == \"\"]\n",
    "FeatureMatrix= FeatureMatrix.drop(columns= \"GNPS_IDs\")\n",
    "FeatureMatrix= FeatureMatrix.set_index([\"RT\", \"mz\"])\n",
    "FeatureMatrix_tocsv= FeatureMatrix.reset_index()\n",
    "FeatureMatrix_tocsv.to_csv(os.path.join(\"results\", \"annotations\", \"FeatureMatrix_unknowns.tsv\"), sep=\"\\t\", index =None)\n",
    "FeatureMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `3) SIRIUS and CSI:FingerID annotations`\n",
    "\n",
    "Create a matrix with all SIRIUS and CSI:FingerID formula and structural predictions, only choose #1 rankings predictions and combine the dataframes to annotate formula and structural predictions according to RT and mz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_formulas = glob.glob(os.path.join(\"results\", \"Sirius\", \"formulas_*.csv\"))\n",
    "input_structures = glob.glob(os.path.join(\"results\", \"Sirius\", \"structures_*.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_formulas will likely contain duplicate formulas that could be either isomeric, isobaric compounds, or identical compounds (with identical RT and mz). Here, we want to collapse the identical, repeating compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_SIRIUS = pd.DataFrame()\n",
    "list_of_df=[]\n",
    "for csv in input_formulas:\n",
    "    df= pd.read_csv(csv, sep=\",\", index_col=\"Unnamed: 0\")\n",
    "    s= df[\"opt_global_rank\"]\n",
    "    pd.to_numeric(s)\n",
    "    df= df.loc[df[\"opt_global_rank\"]==1]\n",
    "    df= df.rename(columns={\"opt_global_featureId\":\"featureId\"})\n",
    "    df_score=df.filter(regex=fr\"Score\")\n",
    "    df_opt=df.filter(regex=fr\"opt\")\n",
    "    cols_score= df_score.columns\n",
    "    cols_opt= df_opt.columns\n",
    "    df= df.drop(columns=cols_score)\n",
    "    df= df.drop(columns= cols_opt)\n",
    "    df=df.reset_index()\n",
    "    list_of_df.append(df)\n",
    "DF_SIRIUS= pd.concat(list_of_df,ignore_index=True)\n",
    "DF_SIRIUS= DF_SIRIUS.drop(columns=\"index\")\n",
    "df_formulas= DF_SIRIUS.rename(columns= {\"chemical_formula\": \"formulas\", \"exp_mass_to_charge\": \"mz\", \"retention_time\": \"RT\"})\n",
    "df_formulas = df_formulas.set_index(\"formulas\")\n",
    "df_singletons=df_formulas.reset_index().drop_duplicates(subset=\"formulas\", keep=False)\n",
    "\n",
    "df_singletons= df_singletons.set_index(\"formulas\")\n",
    "idx= df_singletons.index\n",
    "df_sirius= df_formulas.drop(idx) #drop the singletons and keep the duplicated formulas\n",
    "new_df= pd.DataFrame() #create new, empty DF\n",
    "df= pd.DataFrame() #create new, empty DF\n",
    "idx= df_sirius.index #index of DF with duplicates\n",
    "for i, index in enumerate(idx): #parse through the index\n",
    "    new_index= new_df.index #create a new index\n",
    "    if index not in new_index: #if the old index is not already appended in the new one:\n",
    "        s= df_sirius.iloc[i] #get the whole row of the old DF\n",
    "        new_df= new_df.append(s) #and append it to the new DF\n",
    "    else: #if it already is then check if the features are identical (Delta mz<10ppm, Delta RT+- 30s)\n",
    "        for j, mz_1, time_1 in zip(new_df.index, new_df[\"mz\"], new_df[\"RT\"]):\n",
    "            ids=[]\n",
    "            for mz_0, time_0, id_0 in zip(df_sirius[\"mz\"], df_sirius[\"RT\"], df_sirius[\"featureId\"]):\n",
    "                mass_delta = (abs(mz_0 - mz_1)/mz_0)*1000000\n",
    "                maxdeltaRT = time_0 + 30.0\n",
    "                mindeltaRT = time_0 - 30.0\n",
    "                if (mindeltaRT<= time_1 <= maxdeltaRT) & (mass_delta<= 10.0):\n",
    "                    id= id_0\n",
    "                    if id not in ids:\n",
    "                        ids.append(id)\n",
    "            new_df[\"featureId\"][j] = \" , \".join(ids) #if they are identical, append the feature ids only under the featureId column \n",
    "        else: #if they are not identical\n",
    "            m= df_sirius.iloc[i]\n",
    "            df= df.append(m)\n",
    "\n",
    "DF_SIRIUS= pd.concat([new_df, df], axis=0)\n",
    "DF_SIRIUS_final= pd.concat([DF_SIRIUS, df_singletons], axis=0)\n",
    "DF_SIRIUS_final= DF_SIRIUS_final.reset_index()\n",
    "DF_SIRIUS_final= DF_SIRIUS_final.rename(columns={\"index\":\"formulas\"})\n",
    "DF_SIRIUS_final[\"featureId\"]= DF_SIRIUS_final[\"featureId\"].str.replace(r\"id_\", \"\")\n",
    "for i, rows in DF_SIRIUS_final.iterrows():\n",
    "    DF_SIRIUS_final[\"featureId\"][i]= DF_SIRIUS_final[\"featureId\"][i].split(\",\")\n",
    "DF_SIRIUS_final.to_csv(os.path.join(\"results\", \"annotations\", \"SIRIUS_library.csv\"), sep=\"\\t\", index=None)\n",
    "DF_SIRIUS_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for structral predictions (remove duplicates with the same inchi_keys, which means they represent the same structure):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_CSI= []\n",
    "for i, formulas in enumerate(input_structures):\n",
    "    df= pd.read_csv(formulas, index_col=\"Unnamed: 0\")\n",
    "    df= df.loc[df[\"opt_global_rank\"]==1]\n",
    "    df_score=df.filter(regex=fr\"best_search_engine_score\")\n",
    "    df= df.rename(columns={\"opt_global_featureId\":\"featureId\"})\n",
    "    df_opt=df.filter(regex=fr\"opt\")\n",
    "    cols_score= df_score.columns\n",
    "    cols_opt= df_opt.columns\n",
    "    df= df.drop(columns=cols_score)\n",
    "    df= df.drop(columns= cols_opt)\n",
    "    df= df.drop(columns= \"identifier\")\n",
    "    df=df.reset_index()\n",
    "    df= df.drop(columns=\"index\")\n",
    "    DF_CSI.append(df)\n",
    "\n",
    "\n",
    "df_structures= pd.concat(DF_CSI, axis=0).sort_values(\"chemical_formula\")\n",
    "df_structures = df_structures.drop_duplicates(subset=[\"inchi_key\"], keep=\"first\")\n",
    "df_structures= df_structures.drop(columns=[\"inchi_key\"]) #leave smiles for visualisationdf_structures= df_structures.rename(columns={\"chemical_formula\": \"formulas\", \"exp_mass_to_charge\": \"mz\", \"retention_time\": \"RT\"})\n",
    "df_structures= df_structures.rename(columns={\"chemical_formula\":\"formulas\"})\n",
    "df_structures= df_structures.set_index(\"formulas\")\n",
    "df_singletons=df_structures.reset_index().drop_duplicates(subset=\"formulas\", keep=False)\n",
    "df_singletons= df_singletons.set_index(\"formulas\")\n",
    "idx= df_singletons.index\n",
    "df_CSI= df_structures.drop(labels=idx, axis=0)\n",
    "new_df= pd.DataFrame()\n",
    "df= pd.DataFrame()\n",
    "idx= df_CSI.index\n",
    "for i, index in enumerate(idx):\n",
    "    new_index= new_df.index\n",
    "    if index not in new_index:\n",
    "        s= df_CSI.iloc[i]\n",
    "        new_df= new_df.append(s)\n",
    "    else: #if it already is then check if the features are identical (Delta mz<10ppm, Delta RT+- 30s)\n",
    "        for j, mz_1, time_1 in zip(new_df.index, new_df[\"mz\"], new_df[\"RT\"]):\n",
    "            ids=[]\n",
    "            for mz_0, time_0, id_0 in zip(df_CSI[\"mz\"], df_CSI[\"RT\"], df_CSI[\"featureId\"]):\n",
    "                mass_delta = (abs(mz_0 - mz_1)/mz_0)*1000000\n",
    "                maxdeltaRT = time_0 + 30.0\n",
    "                mindeltaRT = time_0 - 30.0\n",
    "                if (mindeltaRT<= time_1 <= maxdeltaRT) & (mass_delta<= 10.0):\n",
    "                    id= id_0\n",
    "                    if id not in ids:\n",
    "                        ids.append(id)\n",
    "            new_df[\"featureId\"][j] = \" , \".join(ids) #if they are identical, append the feature ids only under the featureId column \n",
    "        else: #if they are not identical\n",
    "            m= df_CSI.iloc[i]\n",
    "            df= df.append(m)\n",
    "\n",
    "DF_CSI= pd.concat([new_df, df], axis=0)\n",
    "DF_CSI_final= pd.concat([DF_CSI, df_singletons], axis=0)\n",
    "DF_CSI_final= DF_CSI_final.reset_index()\n",
    "DF_CSI_final= DF_CSI_final.rename(columns={\"index\":\"formulas\"})\n",
    "DF_CSI_final[\"featureId\"]= DF_CSI_final[\"featureId\"].str.replace(r\"id_\", \"\")\n",
    "for i, rows in DF_CSI_final.iterrows():\n",
    "    DF_CSI_final[\"featureId\"][i]= DF_CSI_final[\"featureId\"][i].split(\",\")\n",
    "DF_CSI_final.to_csv(os.path.join(\"results\", \"annotations\", \"CSI_library.csv\"), sep=\"\\t\", index= None)\n",
    "DF_CSI_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_features= DF_features.reset_index()\n",
    "DF_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotate the formulas and structural predictions to the feature matrix according to SIRIUS and CSI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_features.insert(0, \"CSI_predictions_name\", \"\")\n",
    "DF_features.insert(0, \"CSI_predictions_formula\", \"\")\n",
    "DF_features.insert(0, \"CSI_predictions_smiles\", \"\")\n",
    "\n",
    "\n",
    "for i, id in zip(DF_features.index, DF_features[\"id_list\"]):\n",
    "    hits1 = []\n",
    "    hits2= []\n",
    "    hits3=[]\n",
    "    for name, smiles, formula, CSI_ids in zip(DF_CSI_final[\"description\"], DF_CSI_final[\"smiles\"], DF_CSI_final[\"formulas\"], DF_CSI_final[\"featureId\"]):\n",
    "        for x in id:\n",
    "            if x in CSI_ids:\n",
    "                hit1 = f\"{name}\"\n",
    "                hit2 = f\"{formula}\"\n",
    "                hit3= f\"{smiles}\"\n",
    "                if hit1 not in hits1:\n",
    "                    hits1.append(hit1)\n",
    "                    hits2.append(hit2)\n",
    "                    hits3.append(hit3)\n",
    "    DF_features[\"CSI_predictions_name\"][i] = \" ## \".join(hits1)\n",
    "    DF_features[\"CSI_predictions_formula\"][i] = \" ## \".join(hits2)\n",
    "    DF_features[\"CSI_predictions_smiles\"][i] = \" ## \".join(hits3)\n",
    "DF_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_features.insert(0, \"SIRIUS_predictions\", \"\")\n",
    "\n",
    "for i, id in zip(DF_features.index, DF_features[\"id_list\"]):\n",
    "    hits = []\n",
    "    for name, Pred_id in zip(DF_SIRIUS_final[\"formulas\"], DF_SIRIUS_final[\"featureId\"]): \n",
    "        for x in id:\n",
    "            if x in Pred_id:\n",
    "                hit = f\"{name}\"\n",
    "                if hit not in hits:\n",
    "                    hits.append(hit)\n",
    "        DF_features[\"SIRIUS_predictions\"][i] = \" ## \".join(hits)\n",
    "DF_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_features.to_csv(os.path.join(\"results\", \"annotations\", \"FeatureMatrix_SIRIUS_CSI.csv\"), sep=\"\\t\", index= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "edde62aa2661007f0756e9790e7a328c288a583bf6ce768a355147dac67c8db8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pyopenms')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
