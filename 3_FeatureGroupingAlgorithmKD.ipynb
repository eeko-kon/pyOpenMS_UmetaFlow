{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Determination of memory status is not supported on this \n",
      " platform, measuring for memoryleaks will never fail\n"
     ]
    }
   ],
   "source": [
    "from pyopenms import *\n",
    "import pyopenms as pms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsensusMapDF(ConsensusMap):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_intensity_df(self):\n",
    "        labelfree = self.getExperimentType() == \"label-free\"\n",
    "        filemeta = self.getColumnHeaders()  # type: dict[int, ColumnHeader]\n",
    "        labels = list(set([header.label for header in\n",
    "                           filemeta.values()]))  # TODO could be more efficient. Do we require same channels in all files?\n",
    "        files = list(set([header.filename for header in filemeta.values()]))\n",
    "        label_to_idx = {k: v for v, k in enumerate(labels)}\n",
    "        file_to_idx = {k: v for v, k in enumerate(files)}\n",
    "\n",
    "        def gen(cmap: ConsensusMap, fun):\n",
    "            for f in cmap:\n",
    "                yield from fun(f)\n",
    "\n",
    "        if not labelfree:\n",
    "            # TODO write two functions for LF and labelled. One has only one channel, the other has only one file per CF\n",
    "            def extractRowBlocksChannelWideFileLong(f: ConsensusFeature):\n",
    "                subfeatures = f.getFeatureList()  # type: list[FeatureHandle]\n",
    "                filerows = defaultdict(lambda: [0] * len(labels))  # TODO use numpy array?\n",
    "                for fh in subfeatures:\n",
    "                    header = filemeta[fh.getMapIndex()]\n",
    "                    row = filerows[header.filename]\n",
    "                    row[label_to_idx[header.label]] = fh.getIntensity()\n",
    "                return (f.getUniqueId(), filerows)\n",
    "\n",
    "            def extractRowsChannelWideFileLong(f: ConsensusFeature):\n",
    "                uniqueid, rowdict = extractRowBlocksChannelWideFileLong(f)\n",
    "                for file, row in rowdict.items():\n",
    "                    row.append(file)\n",
    "                    yield tuple([uniqueid] + row)\n",
    "\n",
    "            if len(labels) == 1:\n",
    "                labels[0] = \"intensity\"\n",
    "            dtypes = [('id', np.dtype('uint64'))] + list(zip(labels, ['f'] * len(labels)))\n",
    "            dtypes.append(('file', 'U300'))\n",
    "            # For TMT we know that every feature can only be from one file, since feature = PSM\n",
    "            #cnt = 0\n",
    "            #for f in self:\n",
    "            #    cnt += f.size()\n",
    "\n",
    "            intyarr = np.fromiter(iter=gen(self, extractRowsChannelWideFileLong), dtype=dtypes, count=self.size())\n",
    "            return pd.DataFrame(intyarr).set_index('id')\n",
    "        else:\n",
    "            # Specialized for LabelFree which has to have only one channel\n",
    "            def extractRowBlocksChannelLongFileWideLF(f: ConsensusFeature):\n",
    "                subfeatures = f.getFeatureList()  # type: list[FeatureHandle]\n",
    "                row = [0.] * len(files)  # TODO use numpy array?\n",
    "                for fh in subfeatures:\n",
    "                    header = filemeta[fh.getMapIndex()]\n",
    "                    row[file_to_idx[header.filename]] = fh.getIntensity()\n",
    "                yield tuple([f.getUniqueId()] + row)\n",
    "\n",
    "            dtypes = [('id', np.dtype('uint64'))] + list(zip(files, ['f'] * len(files)))\n",
    "            # cnt = self.size()*len(files) # TODO for this to work, we would need to fill with NAs for CFs that do not go over all files\n",
    "            cnt = self.size()\n",
    "\n",
    "            intyarr = np.fromiter(iter=gen(self, extractRowBlocksChannelLongFileWideLF), dtype=dtypes, count=cnt)\n",
    "            return pd.DataFrame(intyarr).set_index('id')\n",
    "\n",
    "    def get_metadata_df(self):\n",
    "        def gen(cmap: ConsensusMap, fun):\n",
    "            for f in cmap:\n",
    "                yield from fun(f)\n",
    "\n",
    "        def extractMetaData(f: ConsensusFeature):\n",
    "            # subfeatures = f.getFeatureList()  # type: list[FeatureHandle]\n",
    "            pep = f.getPeptideIdentifications()  # type: list[PeptideIdentification]\n",
    "            if len(pep) != 0:\n",
    "                hits = pep[0].getHits()\n",
    "                if len(hits) != 0:\n",
    "                    besthit = hits[0]  # type: PeptideHit\n",
    "                    # TODO what else\n",
    "                    yield f.getUniqueId(), besthit.getSequence().toString(), f.getCharge(), f.getRT(), f.getMZ(), f.getQuality()\n",
    "                else:\n",
    "                    yield f.getUniqueId(), None, f.getCharge(), f.getRT(), f.getMZ(), f.getQuality()\n",
    "            else:\n",
    "                yield f.getUniqueId(), None, f.getCharge(), f.getRT(), f.getMZ(), f.getQuality()\n",
    "\n",
    "        cnt = self.size()\n",
    "\n",
    "        mddtypes = [('id', np.dtype('uint64')), ('sequence', 'U200'), ('charge', 'i4'), ('RT', 'f'), ('mz', 'f'),\n",
    "                    ('quality', 'f')]\n",
    "        mdarr = np.fromiter(iter=gen(self, extractMetaData), dtype=mddtypes, count=cnt)\n",
    "        return pd.DataFrame(mdarr).set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['results/consensus/interim/IDMapper_20210827_UMETAB219_POS_DNPM_Plate-2_MDNAWGS14_blank.featureXML', 'results/consensus/interim/IDMapper_20210827_UMETAB219_POS_DNPM_Plate-2_MDNAWGS14_rep1.featureXML', 'results/consensus/interim/IDMapper_20210827_UMETAB219_POS_DNPM_Plate-2_MDNAWGS14_rep2.featureXML', 'results/consensus/interim/IDMapper_20210827_UMETAB219_POS_DNPM_Plate-2_MDNAWGS14_rep3.featureXML', 'results/consensus/interim/IDMapper_20210827_UMETAB219_POS_FPY12_Plate-2_MDNAWGS14_blank.featureXML', 'results/consensus/interim/IDMapper_20210827_UMETAB219_POS_FPY12_Plate-2_MDNAWGS14_rep1.featureXML', 'results/consensus/interim/IDMapper_20210827_UMETAB219_POS_FPY12_Plate-2_MDNAWGS14_rep2.featureXML', 'results/consensus/interim/IDMapper_20210827_UMETAB219_POS_FPY12_Plate-2_MDNAWGS14_rep3.featureXML', 'results/consensus/interim/IDMapper_20210827_UMETAB219_POS_ISP2_Plate-2_MDNAWGS14_blank.featureXML', 'results/consensus/interim/IDMapper_20210827_UMETAB219_POS_ISP2_Plate-2_MDNAWGS14_rep1.featureXML', 'results/consensus/interim/IDMapper_20210827_UMETAB219_POS_ISP2_Plate-2_MDNAWGS14_rep2.featureXML', 'results/consensus/interim/IDMapper_20210827_UMETAB219_POS_ISP2_Plate-2_MDNAWGS14_rep3.featureXML']\n",
      "results/interim/precursorcorrected_20210827_UMETAB219_POS_DNPM_Plate-2_MDNAWGS14_blank.mzML\n",
      "0\n",
      "results/interim/precursorcorrected_20210827_UMETAB219_POS_DNPM_Plate-2_MDNAWGS14_rep1.mzML\n",
      "0\n",
      "results/interim/precursorcorrected_20210827_UMETAB219_POS_DNPM_Plate-2_MDNAWGS14_rep2.mzML\n",
      "0\n",
      "results/interim/precursorcorrected_20210827_UMETAB219_POS_DNPM_Plate-2_MDNAWGS14_rep3.mzML\n",
      "0\n",
      "results/interim/precursorcorrected_20210827_UMETAB219_POS_FPY12_Plate-2_MDNAWGS14_blank.mzML\n",
      "0\n",
      "results/interim/precursorcorrected_20210827_UMETAB219_POS_FPY12_Plate-2_MDNAWGS14_rep1.mzML\n",
      "0\n",
      "results/interim/precursorcorrected_20210827_UMETAB219_POS_FPY12_Plate-2_MDNAWGS14_rep2.mzML\n",
      "0\n",
      "results/interim/precursorcorrected_20210827_UMETAB219_POS_FPY12_Plate-2_MDNAWGS14_rep3.mzML\n",
      "0\n",
      "results/interim/precursorcorrected_20210827_UMETAB219_POS_ISP2_Plate-2_MDNAWGS14_blank.mzML\n",
      "0\n",
      "results/interim/precursorcorrected_20210827_UMETAB219_POS_ISP2_Plate-2_MDNAWGS14_rep1.mzML\n",
      "0\n",
      "results/interim/precursorcorrected_20210827_UMETAB219_POS_ISP2_Plate-2_MDNAWGS14_rep2.mzML\n",
      "0\n",
      "results/interim/precursorcorrected_20210827_UMETAB219_POS_ISP2_Plate-2_MDNAWGS14_rep3.mzML\n",
      "0\n",
      "Progress of 'computing RT transformations':\n",
      "-- done [took 0.12 s (CPU), 0.11 s (Wall)] -- \n",
      "Progress of 'linking features':\n",
      "-- done [took 0.25 s (CPU), 0.25 s (Wall)] -- \n",
      "ConsensusXMLFile::store():  found 8873 invalid unique ids\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "feature_grouper = FeatureGroupingAlgorithmKD()\n",
    "\n",
    "consensus_map = ConsensusMapDF()\n",
    "file_descriptions = consensus_map.getColumnHeaders()\n",
    "\n",
    "input_feature_files = sorted(glob.glob('results/GNPSexport/interim/IDMapper*.featureXML'))\n",
    "print(input_feature_files)\n",
    "\n",
    "feature_maps = []\n",
    "for featurexml_file in input_feature_files:\n",
    "    fmap = FeatureMap()\n",
    "    FeatureXMLFile().load(featurexml_file, fmap)\n",
    "    feature_maps.append(fmap)\n",
    "\n",
    "for i, feature_map in enumerate(feature_maps):\n",
    "    file_description = file_descriptions.get(i, ColumnHeader())\n",
    "    file_description.filename = feature_map.getMetaValue('spectra_data')[0].decode()\n",
    "    print(file_description.filename)\n",
    "\n",
    "    file_description.size = feature_map.size()\n",
    "    # file_description.unique_id = feature_map.getUniqueId() doesn't work on windows\n",
    "    print(file_description.unique_id)\n",
    "    file_descriptions[i] = file_description\n",
    "\n",
    "feature_grouper.group(feature_maps, consensus_map)\n",
    "consensus_map.setColumnHeaders(file_descriptions)\n",
    "\n",
    "\n",
    "Consensus_file= os.path.join(\"results\", \"\", \"GNPSexport\", \"\",\"interim\", \"\", 'consensus' + \".consensusXML\")\n",
    "ConsensusXMLFile().store(Consensus_file, consensus_map)\n",
    "\n",
    "\n",
    "# get intensities as a DataFrame\n",
    "intensities = consensus_map.get_intensity_df()\n",
    "\n",
    "# get meta data as DataFrame\n",
    "meta_data = consensus_map.get_metadata_df()[['RT', 'mz', 'charge']]\n",
    "\n",
    "# you can concatenate these two for a \"result\" DataFrame\n",
    "result = pd.concat([meta_data, intensities], axis=1)\n",
    "\n",
    "# if you don't need labeled index, remove it (and/or save with index = False)\n",
    "result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# store as tsv file\n",
    "result.to_csv('results/GNPSexport/interim/Consensus.tsv', sep = '\\t', index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "edde62aa2661007f0756e9790e7a328c288a583bf6ce768a355147dac67c8db8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
